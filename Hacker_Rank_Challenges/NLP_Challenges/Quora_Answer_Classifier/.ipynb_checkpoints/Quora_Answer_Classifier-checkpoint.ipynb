{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacker Rank Link:\n",
    "# https://www.hackerrank.com/challenges/quora-answer-classifier/problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quora uses a combination of machine learning (ML) algorithms and moderation to ensure high-quality content on the site. High answer quality has helped Quora distinguish itself from other Q&A sites on the web.  \n",
    "\n",
    "Your task will be to devise a classifier that is able to tell good answers from bad answers, as well as humans can.  A good answer is denoted by a +1 in our system, and a bad answer is denoted by a -1.\n",
    "\n",
    "Input Format:\n",
    "\n",
    "The first line contains N, M. N = Number of training data records, M = number of parameters. Followed by N lines containing records of training data. Then one integer q, q = number of records to be classified, followed by q lines of query data\n",
    "Training data corresponds to the following format:\n",
    "(:)*\n",
    "\n",
    "Query data corresponds to the following format:\n",
    "(:)*\n",
    "\n",
    "The answer identifier  is an alphanumeric string of no more than 10 chars.  Each identifier is guaranteed unique.  All feature values are doubles.\n",
    "\n",
    "The data is extracted from real production data, and thus will not include the raw form of the answers. We, however, have extracted as many features as we think are useful, and you can decide which features make sense to be included in your final algorithm. The actual labelling of a good answer and bad answer is done organically on our site, through both human moderators as well as our own classifier.\n",
    "\n",
    "Output Format:\n",
    "\n",
    "For each query, you should output q lines to stdout, representing the decision made by your classifier, whether each answer is good or not:\n",
    "Constraints:\n",
    "\n",
    "0 0 0\n",
    "Sample Input:\n",
    "\n",
    "5 23\n",
    "2LuzC +1 1:2101216030446 2:1.807711 3:1 4:4.262680 5:4.488636 6:87.000000 7:0.000000 8:0.000000 9:0 10:0 11:3.891820 12:0 13:1 14:0 15:0 16:0 17:1 18:1 19:0 20:2 21:2.197225 22:0.000000 23:0.000000\n",
    "LmnUc +1 1:99548723068 2:3.032810 3:1 4:2.772589 5:2.708050 6:0.000000 7:0.000000 8:0.000000 9:0 10:0 11:4.727388 12:5 13:1 14:0 15:0 16:1 17:1 18:0 19:0 20:9 21:2.833213 22:0.000000 23:0.000000\n",
    "ZINTz -1 1:3030695193589 2:1.741764 3:1 4:2.708050 5:4.248495 6:0.000000 7:0.000000 8:0.000000 9:0 10:0 11:3.091042 12:1 13:1 14:0 15:0 16:0 17:1 18:1 19:0 20:5 21:2.564949 22:0.000000 23:0.000000\n",
    "gX60q +1 1:2086220371355 2:1.774193 3:1 4:3.258097 5:3.784190 6:0.000000 7:0.000000 8:0.000000 9:0 10:0 11:3.258097 12:0 13:1 14:0 15:0 16:0 17:1 18:0 19:0 20:5 21:2.995732 22:0.000000 23:0.000000\n",
    "5HG4U -1 1:352013287143 2:1.689824 3:1 4:0.000000 5:0.693147 6:0.000000 7:0.000000 8:0.000000 9:0 10:1 11:1.791759 12:0 13:1 14:1 15:0 16:1 17:0 18:0 19:0 20:4 21:2.197225 22:0.000000 23:0.000000\n",
    "2\n",
    "PdxMK 1:340674897225 2:1.744152 3:1 4:5.023881 5:7.042286 6:0.000000 7:0.000000 8:0.000000 9:0 10:0 11:3.367296 12:0 13:1 14:0 15:0 16:0 17:0 18:0 19:0 20:12 21:4.499810 22:0.000000 23:0.000000\n",
    "ehZ0a 1:2090062840058 2:1.939101 3:1 4:3.258097 5:2.995732 6:75.000000 7:0.000000 8:0.000000 9:0 10:0 11:3.433987 12:0 13:1 14:0 15:0 16:1 17:0 18:0 19:0 20:3 21:2.639057 22:0.000000 23:0.000000\n",
    "\n",
    "\n",
    "\n",
    "Sample Output:\n",
    "\n",
    "PdxMK +1\n",
    "ehZ0a -1\n",
    "\n",
    "You will be given a relative large input dataset with its corresponding output to finetune your program with your ML libraries.\n",
    "\n",
    "Scoring\n",
    "Only one very large dataset will be given for this problem as input to your program for scoring.  This input data set will not be revealed to you.\n",
    "\n",
    "Output for every classification is awarded points separately. The score for this problem will be the sum of points for each correct classification. To prevent naive solution credit (outputting all +1s, for example), points are awarded only after X correct classifications, where X is number of +1 answers or -1 answers (whichever is greater).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when reading from Hacker Rank\n",
    "\n",
    "# import fileinput \n",
    "\n",
    "# temp = []  \n",
    "    \n",
    "# for f in fileinput.input(): \n",
    "#     temp.append(f)\n",
    "    \n",
    "# df = pd.DataFrame(temp)\n",
    "# df[0] = df[0].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0                                            4500 23\n",
      "1  Nt8FJ +1 1:12087620705283 2:4.797982 3:1 4:4.9...\n",
      "2  VCaTF +1 1:282114466020 2:3.151926 3:1 4:3.737...\n",
      "3  gParY +1 1:173284955 2:1.785813 3:1 4:1.791759...\n",
      "4  DtWDw +1 1:4708728355523 2:2.394989 3:1 4:3.09...\n"
     ]
    }
   ],
   "source": [
    "# when reading locally\n",
    "df = pd.read_csv('input00.txt', header= None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_line = df[0][0]\n",
    "feature_num = int(first_line.split(' ')[1])\n",
    "train_cnt = int(first_line.split(' ')[0])\n",
    "input_cnt = df[0][train_cnt+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_validation = pd.DataFrame(df[1:train_cnt+1][0].str.split(\" \").tolist())\n",
    "\n",
    "df_train, df_validation = train_test_split(df_train_validation, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training_validation(data):\n",
    "    y = data[1].astype(float)\n",
    "    y = y.replace(-1, 0)\n",
    "    X = data.drop(data.columns[[0, 1]], axis=1)\n",
    "    X = X.T.reset_index(drop=True).T\n",
    "    \n",
    "    for i in range(feature_num):\n",
    "        X[i] = X[i].str.split(\":\", expand = True)[1]\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = process_training_validation(df_train)\n",
    "X_validation, y_validation = process_training_validation(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.DataFrame(df[train_cnt+2:][0].str.split(\" \").tolist())\n",
    "\n",
    "input_features = df_input.drop(df_input.columns[0], axis=1)\n",
    "\n",
    "input_features = input_features.T.reset_index(drop=True).T\n",
    "\n",
    "\n",
    "for i in range(feature_num):\n",
    "    input_features[i] = input_features[i].str.split(\":\", expand = True)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(method):\n",
    "    model = method\n",
    "    model.fit(X_train, y_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    print (classification_report(y_validation, y_validation_pred))\n",
    "    print (\"The accuracy score is {:.2%}\".format(accuracy_score(y_validation, y_validation_pred)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       439\n",
      "         1.0       0.51      1.00      0.68       461\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       900\n",
      "   macro avg       0.26      0.50      0.34       900\n",
      "weighted avg       0.26      0.51      0.35       900\n",
      "\n",
      "The accuracy score is 51.22%\n"
     ]
    }
   ],
   "source": [
    "model_LogisticRegression = evaluate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.83       439\n",
      "         1.0       0.85      0.82      0.83       461\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       900\n",
      "   macro avg       0.83      0.83      0.83       900\n",
      "weighted avg       0.83      0.83      0.83       900\n",
      "\n",
      "The accuracy score is 83.11%\n"
     ]
    }
   ],
   "source": [
    "model_RandomForest = evaluate_model(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      1.00      0.66       439\n",
      "         1.0       0.00      0.00      0.00       461\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       900\n",
      "   macro avg       0.24      0.50      0.33       900\n",
      "weighted avg       0.24      0.49      0.32       900\n",
      "\n",
      "The accuracy score is 48.78%\n"
     ]
    }
   ],
   "source": [
    "model_SGDC = evaluate_model(SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      1.00      0.66       439\n",
      "         1.0       0.00      0.00      0.00       461\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       900\n",
      "   macro avg       0.24      0.50      0.33       900\n",
      "weighted avg       0.24      0.49      0.32       900\n",
      "\n",
      "The accuracy score is 48.78%\n"
     ]
    }
   ],
   "source": [
    "model_SVC = evaluate_model(LinearSVC(C=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC is too slow\n",
    "# model_SVC = evaluate_model(SVC(kernel = 'linear', C=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.46      0.52       439\n",
      "         1.0       0.57      0.69      0.63       461\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       900\n",
      "   macro avg       0.58      0.58      0.57       900\n",
      "weighted avg       0.58      0.58      0.57       900\n",
      "\n",
      "The accuracy score is 58.00%\n"
     ]
    }
   ],
   "source": [
    "model_MultinomialNB = evaluate_model(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.13      0.19       439\n",
      "         1.0       0.50      0.84      0.63       461\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       900\n",
      "   macro avg       0.47      0.48      0.41       900\n",
      "weighted avg       0.47      0.49      0.42       900\n",
      "\n",
      "The accuracy score is 49.33%\n"
     ]
    }
   ],
   "source": [
    "model_GaussianNB = evaluate_model(GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      1.00      0.66       439\n",
      "         1.0       0.00      0.00      0.00       461\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       900\n",
      "   macro avg       0.24      0.50      0.33       900\n",
      "weighted avg       0.24      0.49      0.32       900\n",
      "\n",
      "The accuracy score is 48.78%\n"
     ]
    }
   ],
   "source": [
    "model_SGDC = evaluate_model(SGDClassifier( loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=8, tol=None ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.84       439\n",
      "         1.0       0.84      0.84      0.84       461\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       900\n",
      "   macro avg       0.84      0.84      0.84       900\n",
      "weighted avg       0.84      0.84      0.84       900\n",
      "\n",
      "The accuracy score is 83.89%\n"
     ]
    }
   ],
   "source": [
    "model_GradientBoosting = evaluate_model(GradientBoostingClassifier( n_estimators=200, max_depth=3, learning_rate=0.3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3rCWr -1', 'snInN -1', 'ibfT7 +1', 'IcbKR +1', 'SIXmF +1', 'dLCdh +1', 'ziFJ8 -1', '1WtTD -1', '9uIKh +1', 'df4Mc -1', '3nxpY -1', 'aesmq +1', 'MyTDz +1', 'TDMhx +1', 'Y0rW3 +1', 'KCcKf +1', '2cz5M -1', 'kqIJj -1', 'C1Sg2 +1', 'VAmIt -1']\n"
     ]
    }
   ],
   "source": [
    "name_list = [i[0] for i in df[train_cnt+2:][0].str.split(\" \")]\n",
    "\n",
    "prediction_list = model_RandomForest.predict(input_features)\n",
    "\n",
    "final_prediction_list = []\n",
    "\n",
    "\n",
    "for i in range(len(prediction_list)):\n",
    "    if prediction_list[i] == 1:\n",
    "        final_prediction_list.append(name_list[i] + \" \" + '+1')\n",
    "#         print(name_list[i] + \" \" + '+1')\n",
    "    else:\n",
    "        final_prediction_list.append(name_list[i] + \" \" + '-1')\n",
    "#         print(name_list[i] + \" \" + '-1')\n",
    "    \n",
    "print(final_prediction_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaway:\n",
    "Only tree based models have good performance. The reason could be the linearity assumption do not hold true!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
