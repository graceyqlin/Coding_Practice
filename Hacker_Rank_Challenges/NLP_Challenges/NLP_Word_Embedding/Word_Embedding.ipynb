{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus') as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "corpus_list = []\n",
    "for line in corpus.split('\\n'):\n",
    "    corpus_list.append(line)\n",
    "\n",
    "label_length = len('__label__2')\n",
    "y_list = []\n",
    "X_list = []\n",
    "for document in corpus_list:\n",
    "    y_list.append(document[:label_length])\n",
    "    X_list.append(document[label_length+1:])\n",
    "\n",
    "processed_y_list = []\n",
    "for i in y_list:\n",
    "    if i == '__label__2':\n",
    "        processed_y_list.append(2)\n",
    "    else:\n",
    "        processed_y_list.append(1)\n",
    "        \n",
    "y_list = processed_y_list\n",
    "\n",
    "y_list = np.array(y_list)\n",
    "\n",
    "y_list = y_list.reshape(-1, 1)\n",
    "\n",
    "y_list = y_list.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pretrained model from Spacy\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# use a smaller X for demenstration purpose\n",
    "\n",
    "Small_X_list = X_list[:500]\n",
    "\n",
    "word2vec_X = [nlp(sentence).vector for sentence in Small_X_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(input_list):\n",
    "    print('the shape is', len(input_list), len(input_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape is 500 96\n"
     ]
    }
   ],
   "source": [
    "print_shape(word2vec_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Added Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "doc = []\n",
    "for line in open('glove.6B.100d.txt', 'r'):\n",
    "    doc.append(line.replace('\\n', ''))\n",
    "\n",
    "glove_dict = defaultdict(list)\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "for line in doc:\n",
    "    word = line.split(' ')[0]\n",
    "    vector = np.asarray(line.split(' ')[1:], dtype='float32')\n",
    "    glove_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentence):\n",
    "    \n",
    "    sentece_words = str(sentence).lower()\n",
    "    sentece_words = word_tokenize(sentece_words)\n",
    "    sentece_words = [word for word in sentece_words if not word in stopwords]\n",
    "    sentece_words = [word for word in sentece_words if word.isalpha()]\n",
    "    \n",
    "    sentence_word_vectors = []\n",
    "    \n",
    "    for word in sentece_words:\n",
    "        sentence_word_vectors.append(glove_dict[word])\n",
    "            \n",
    "    sentence_word_vectors = np.array(sentence_word_vectors)\n",
    "    \n",
    "    sum_sentence_word_vectors = sentence_word_vectors.sum(axis=0)\n",
    "    \n",
    "    if type(sum_sentence_word_vectors) != np.ndarray:\n",
    "        \n",
    "        return np.zeros(100)\n",
    "    \n",
    "    # 100 is because the data we are using is 100 dimension\n",
    "    \n",
    "    normalized_sentence_word_vectors = sum_sentence_word_vectors / len(sentence_word_vectors)\n",
    "    \n",
    "    return normalized_sentence_word_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.61145954e-02,  3.51955324e-01,  3.86061639e-01, -1.97051018e-01,\n",
       "       -1.72846004e-01,  1.36576906e-01, -1.90544561e-01,  1.33789271e-01,\n",
       "       -1.67788919e-02, -2.71716565e-01,  1.92401558e-01,  3.52062695e-02,\n",
       "        8.22529644e-02, -1.34434909e-01,  6.45520389e-02, -1.62273034e-01,\n",
       "       -6.15987182e-02,  1.54483184e-01, -1.76681414e-01,  4.08039272e-01,\n",
       "        1.99222639e-01,  1.15231499e-01, -5.48733585e-02, -2.06511334e-01,\n",
       "        7.31869563e-02,  3.01482566e-02, -3.14367265e-01, -4.66888577e-01,\n",
       "        2.48672850e-02, -1.58738300e-01, -2.18046710e-01,  4.18308973e-01,\n",
       "       -9.43074841e-03, -1.35022312e-01,  9.99217033e-02,  2.45048061e-01,\n",
       "       -2.90057212e-01, -2.31278595e-02,  3.08087881e-04, -9.92591456e-02,\n",
       "       -2.61613190e-01, -9.07974690e-02,  5.52531220e-02, -4.26564395e-01,\n",
       "       -3.07398975e-01, -1.02058269e-01,  5.85928224e-02, -3.47306103e-01,\n",
       "        1.82511553e-01, -9.18483913e-01,  3.07927072e-01, -1.86705321e-01,\n",
       "       -2.84779239e-02,  9.26880956e-01, -5.42658605e-02, -2.07754230e+00,\n",
       "        1.14633285e-01,  4.11004685e-02,  1.31606853e+00,  1.69402942e-01,\n",
       "       -1.23714246e-01,  7.33622968e-01, -4.63913918e-01, -2.11925238e-01,\n",
       "        6.03594720e-01,  1.68404147e-01,  3.33518088e-01,  3.31299335e-01,\n",
       "       -6.56674281e-02, -1.87220171e-01,  1.27979860e-01, -1.15457870e-01,\n",
       "        3.78423347e-03, -3.52087259e-01,  3.25093903e-02,  2.16964841e-01,\n",
       "       -2.29118690e-01, -1.16789266e-01, -4.68423575e-01, -1.93362385e-01,\n",
       "        3.05326790e-01,  3.16528007e-02, -6.04011655e-01, -5.34368046e-02,\n",
       "       -1.34654129e+00, -1.56166941e-01,  4.24547791e-02, -2.33646125e-01,\n",
       "       -3.25311035e-01, -2.19781771e-01,  3.43424678e-02,  2.69647073e-02,\n",
       "        6.86276481e-02, -2.78514445e-01, -4.64152426e-01,  1.31259756e-02,\n",
       "        2.05212124e-02, -1.24531999e-01,  3.13805908e-01,  4.65887934e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2vec(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted weights based on corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X_list, test_size = 0.2)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "X_train_vectorized = tfidf_vectorizer.transform(X_train)\n",
    "X_test_vectorized = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8000x28234 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 443569 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not adjust weights. just count the frequencies\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(X_train)\n",
    "\n",
    "X_train_vectorized = tfidf_vectorizer.transform(X_train)\n",
    "X_test_vectorized = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8000x28234 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 443569 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This embedding purely just encode each word to a number. does not involve any calculation or representation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# first select the 10000 most unique common words\n",
    "max_features = 10000\n",
    "\n",
    "keras_tokenizer = Tokenizer(num_words=max_features)\n",
    "keras_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_vectorized_keras = keras_tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "X_test_vectorized_keras = keras_tokenizer.texts_to_sequences(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure each observation has the same length:\n",
    "# let's assume the maximum word is 500\n",
    "max_word = 500\n",
    "\n",
    "X_train_sequence = pad_sequences(X_train_vectorized_keras, maxlen = max_word)\n",
    "\n",
    "X_test_sequence = pad_sequences(X_test_vectorized_keras, maxlen = max_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape is 8000 500\n"
     ]
    }
   ],
   "source": [
    "print_shape(X_train_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
