{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams \n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "for line in corpus.split('\\n'):\n",
    "    corpus_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_length = len('__label__2')\n",
    "y_list = []\n",
    "X_list = []\n",
    "for document in corpus_list:\n",
    "    y_list.append(document[:label_length])\n",
    "    X_list.append(document[label_length+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(None, 'i'): 1,\n",
       "         ('i', 'ate'): 1,\n",
       "         ('ate', 'an'): 1,\n",
       "         ('an', 'apple'): 1,\n",
       "         ('apple', None): 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'i ate an apple'\n",
    "sentence_word = word_tokenize(sentence)\n",
    "dic = {}\n",
    "new_list = [(w1, w2) for w1, w2 in list(bigrams(sentence_word, pad_right = True, pad_left = True))]\n",
    "collections.Counter(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "\n",
    "for paragraph in X_list:\n",
    "    for sentence in sent_tokenize(paragraph):\n",
    "        sentence_word = word_tokenize(sentence)\n",
    "        for w1, w2 in bigrams(sentence_word, pad_right = True, pad_left = True):\n",
    "            bigram_dict[w1][w2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1 in bigram_dict:\n",
    "    total_w1_cnt = sum(bigram_dict[w1].values())\n",
    "    for w2 in bigram_dict[w1]:\n",
    "        bigram_dict[w1][w2] /= total_w1_cnt        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0494728304947283"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dict['even']['if']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get the next word:\n",
    "def get_next_word(starting_word):\n",
    "    most_freq_word = sorted([(next_word, prob) for (next_word, prob) in bigram_dict[starting_word].items()], \n",
    "                            key = lambda x: -x[1])[0][0]\n",
    "    return most_freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'though'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_word('even')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_word('eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get the most likely sentence:\n",
    "def get_most_likely_sentence(starting_word):\n",
    "    n_iteration = 0\n",
    "    sentence = [starting_word]\n",
    "    while n_iteration < 10:\n",
    "        \n",
    "        next_word = get_next_word(starting_word)\n",
    "        \n",
    "        n_iteration += 1\n",
    "        \n",
    "        sentence.append(next_word)\n",
    "        \n",
    "        if next_word == None:\n",
    "            return sentence\n",
    "        \n",
    "        starting_word = next_word\n",
    "    \n",
    "    final_sentence = ''.join([word for word in sentence if word])\n",
    "  \n",
    "    return final_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['even', 'though', ',', 'and', 'the', 'book', '.', None]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_likely_sentence('even')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oddly', 'into', 'the', 'book', '.', None]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_likely_sentence('oddly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to generate random sentence\n",
    "\n",
    "import random\n",
    "\n",
    "def get_sentence(starting_word):\n",
    "\n",
    "    text = [starting_word]\n",
    "    sentence_finished = False\n",
    "\n",
    "    n_iteration = 0 \n",
    "\n",
    "    while not sentence_finished:\n",
    "        threshold = 0.005\n",
    "        \n",
    "        for word in bigram_dict[text[-1]].keys():\n",
    "            n_iteration += 1\n",
    "            \n",
    "            if n_iteration > 30:\n",
    "                break\n",
    "\n",
    "            if bigram_dict[text[-1]][word] > threshold:\n",
    "                text.append(word)\n",
    "                break\n",
    "\n",
    "            if text[-1] == None:\n",
    "                sentence_finished = True\n",
    "                break\n",
    "\n",
    "        if len(text) > 10:\n",
    "            sentence_finished = True\n",
    "            break\n",
    "\n",
    "#     return ' '.join([t for t in text if t])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['even', 'for', 'the', 'best', 'music', '!', None]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence('even')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, None, 'i'), (None, 'i', 'ate'), ('i', 'ate', 'an'), ('ate', 'an', 'apple'), ('an', 'apple', None), ('apple', None, None)]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'i ate an apple'\n",
    "sentence_word = word_tokenize(sentence)\n",
    "sentence_word_trigram = list(trigrams(sentence_word, pad_left = True, pad_right = True))\n",
    "print(sentence_word_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for paragraph in X_list:\n",
    "    for sentence in sent_tokenize(paragraph):\n",
    "        sentence_word = word_tokenize(sentence)\n",
    "        for w1, w2, w3 in trigrams(sentence_word, pad_right = True, pad_left = True):\n",
    "            trigram_dict[(w1, w2)][w3] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probability\n",
    "\n",
    "for w1, w2 in trigram_dict:\n",
    "    total_cnt = sum(trigram_dict[(w1, w2)].values())\n",
    "    for w3 in trigram_dict[(w1,w2)]:\n",
    "        trigram_dict[(w1, w2)][w3] /= total_cnt\n",
    "    \n",
    "# trigram_dict[('I', 'would')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word_trigram(w1, w2):\n",
    "    next_word = sorted([(w3, prob) for (w3, prob) in trigram_dict[(w1,w2)].items()], key = lambda x: -x[1])[0][0]\n",
    "    return next_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recommend'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_word_trigram('I', 'would')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(text):\n",
    "    length = 30\n",
    "    sequences = list()\n",
    "    for i in range(length, len(text)):\n",
    "        # select sequence of tokens\n",
    "        seq = text[i-length:i+1]\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 396\n"
     ]
    }
   ],
   "source": [
    "# create sequences   \n",
    "sequences = create_seq(X_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stuning even for the non-gamer:',\n",
       " 'tuning even for the non-gamer: ',\n",
       " 'uning even for the non-gamer: T',\n",
       " 'ning even for the non-gamer: Th',\n",
       " 'ing even for the non-gamer: Thi',\n",
       " 'ng even for the non-gamer: This',\n",
       " 'g even for the non-gamer: This ',\n",
       " ' even for the non-gamer: This s',\n",
       " 'even for the non-gamer: This so',\n",
       " 'ven for the non-gamer: This sou']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a character mapping index\n",
    "chars = sorted(list(set(X_list[0])))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "def encode_seq(seq):\n",
    "    sequences = list()\n",
    "    for line in seq:\n",
    "        # integer encode line\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        # store\n",
    "        sequences.append(encoded_seq)\n",
    "    return sequences\n",
    "\n",
    "# encode the sequences\n",
    "sequences = encode_seq(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (356, 30) Val shape: (40, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# vocabulary size\n",
    "vocab = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "# create X and y\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 50)            1650      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 150)               90450     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 33)                4983      \n",
      "=================================================================\n",
      "Total params: 97,083\n",
      "Trainable params: 97,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/lin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 356 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 3.4658 - acc: 0.1601 - val_loss: 3.4287 - val_acc: 0.1750\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.1637 - acc: 0.1910 - val_loss: 3.3001 - val_acc: 0.1750\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.0493 - acc: 0.1910 - val_loss: 3.2314 - val_acc: 0.1750\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.0394 - acc: 0.1910 - val_loss: 3.2147 - val_acc: 0.1750\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.9656 - acc: 0.1910 - val_loss: 3.1759 - val_acc: 0.1750\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.9709 - acc: 0.1910 - val_loss: 3.1926 - val_acc: 0.1750\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.9400 - acc: 0.1910 - val_loss: 3.1863 - val_acc: 0.1750\n",
      "Epoch 8/100\n",
      " - 0s - loss: 2.9213 - acc: 0.1910 - val_loss: 3.1911 - val_acc: 0.1750\n",
      "Epoch 9/100\n",
      " - 0s - loss: 2.8844 - acc: 0.1966 - val_loss: 3.1994 - val_acc: 0.1750\n",
      "Epoch 10/100\n",
      " - 0s - loss: 2.8596 - acc: 0.1938 - val_loss: 3.1428 - val_acc: 0.1750\n",
      "Epoch 11/100\n",
      " - 0s - loss: 2.8180 - acc: 0.1994 - val_loss: 3.1153 - val_acc: 0.1750\n",
      "Epoch 12/100\n",
      " - 0s - loss: 2.7650 - acc: 0.2022 - val_loss: 3.0885 - val_acc: 0.1750\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.7203 - acc: 0.2360 - val_loss: 3.0781 - val_acc: 0.2000\n",
      "Epoch 14/100\n",
      " - 0s - loss: 2.6593 - acc: 0.2612 - val_loss: 3.0330 - val_acc: 0.2000\n",
      "Epoch 15/100\n",
      " - 0s - loss: 2.5930 - acc: 0.2725 - val_loss: 3.0147 - val_acc: 0.2000\n",
      "Epoch 16/100\n",
      " - 0s - loss: 2.5218 - acc: 0.2809 - val_loss: 2.9839 - val_acc: 0.1750\n",
      "Epoch 17/100\n",
      " - 0s - loss: 2.4695 - acc: 0.2978 - val_loss: 2.9602 - val_acc: 0.1500\n",
      "Epoch 18/100\n",
      " - 0s - loss: 2.4103 - acc: 0.2893 - val_loss: 2.9371 - val_acc: 0.1750\n",
      "Epoch 19/100\n",
      " - 0s - loss: 2.3594 - acc: 0.2921 - val_loss: 2.9117 - val_acc: 0.2000\n",
      "Epoch 20/100\n",
      " - 0s - loss: 2.2979 - acc: 0.2949 - val_loss: 2.9054 - val_acc: 0.1750\n",
      "Epoch 21/100\n",
      " - 0s - loss: 2.2535 - acc: 0.3090 - val_loss: 2.9313 - val_acc: 0.1750\n",
      "Epoch 22/100\n",
      " - 0s - loss: 2.1989 - acc: 0.3427 - val_loss: 2.9221 - val_acc: 0.1750\n",
      "Epoch 23/100\n",
      " - 0s - loss: 2.1662 - acc: 0.3427 - val_loss: 2.9425 - val_acc: 0.1750\n",
      "Epoch 24/100\n",
      " - 0s - loss: 2.1225 - acc: 0.3680 - val_loss: 2.9190 - val_acc: 0.2000\n",
      "Epoch 25/100\n",
      " - 0s - loss: 2.0912 - acc: 0.3764 - val_loss: 2.9104 - val_acc: 0.2000\n",
      "Epoch 26/100\n",
      " - 0s - loss: 2.0476 - acc: 0.4017 - val_loss: 2.9503 - val_acc: 0.2000\n",
      "Epoch 27/100\n",
      " - 0s - loss: 2.0047 - acc: 0.3792 - val_loss: 3.0337 - val_acc: 0.2250\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.9678 - acc: 0.4045 - val_loss: 2.9442 - val_acc: 0.2000\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.9272 - acc: 0.4213 - val_loss: 3.0107 - val_acc: 0.2000\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.8766 - acc: 0.4410 - val_loss: 2.9646 - val_acc: 0.2000\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.8402 - acc: 0.4551 - val_loss: 2.9602 - val_acc: 0.2250\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.7822 - acc: 0.4579 - val_loss: 2.9787 - val_acc: 0.2250\n",
      "Epoch 33/100\n",
      " - 1s - loss: 1.7469 - acc: 0.4663 - val_loss: 3.0044 - val_acc: 0.2000\n",
      "Epoch 34/100\n",
      " - 1s - loss: 1.7181 - acc: 0.4775 - val_loss: 3.0920 - val_acc: 0.2500\n",
      "Epoch 35/100\n",
      " - 1s - loss: 1.6649 - acc: 0.4916 - val_loss: 3.0527 - val_acc: 0.2250\n",
      "Epoch 36/100\n",
      " - 1s - loss: 1.6118 - acc: 0.4888 - val_loss: 3.0746 - val_acc: 0.2250\n",
      "Epoch 37/100\n",
      " - 1s - loss: 1.5584 - acc: 0.5281 - val_loss: 3.1118 - val_acc: 0.2250\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.5228 - acc: 0.5281 - val_loss: 3.0941 - val_acc: 0.2750\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.4736 - acc: 0.5281 - val_loss: 3.1232 - val_acc: 0.2500\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.4238 - acc: 0.5646 - val_loss: 3.1680 - val_acc: 0.2500\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.3582 - acc: 0.5927 - val_loss: 3.1995 - val_acc: 0.2750\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.2956 - acc: 0.6011 - val_loss: 3.1998 - val_acc: 0.2750\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.2483 - acc: 0.6264 - val_loss: 3.1858 - val_acc: 0.2500\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.2080 - acc: 0.6404 - val_loss: 3.2362 - val_acc: 0.3000\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.1375 - acc: 0.6601 - val_loss: 3.2618 - val_acc: 0.2500\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.0596 - acc: 0.6882 - val_loss: 3.3323 - val_acc: 0.2500\n",
      "Epoch 47/100\n",
      " - 1s - loss: 1.0378 - acc: 0.6882 - val_loss: 3.4303 - val_acc: 0.2500\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.9785 - acc: 0.7219 - val_loss: 3.3875 - val_acc: 0.2500\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.9150 - acc: 0.7388 - val_loss: 3.3865 - val_acc: 0.2750\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.8730 - acc: 0.7191 - val_loss: 3.5130 - val_acc: 0.2750\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.8440 - acc: 0.7303 - val_loss: 3.5192 - val_acc: 0.2500\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.7896 - acc: 0.7865 - val_loss: 3.5547 - val_acc: 0.2750\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.7082 - acc: 0.8118 - val_loss: 3.6009 - val_acc: 0.2750\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.6847 - acc: 0.8287 - val_loss: 3.7044 - val_acc: 0.3000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.6428 - acc: 0.8399 - val_loss: 3.7579 - val_acc: 0.2750\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.6171 - acc: 0.8427 - val_loss: 3.8485 - val_acc: 0.2750\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.5636 - acc: 0.8624 - val_loss: 3.8144 - val_acc: 0.2750\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.5188 - acc: 0.8876 - val_loss: 3.7258 - val_acc: 0.3500\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4790 - acc: 0.9073 - val_loss: 3.7803 - val_acc: 0.2750\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4557 - acc: 0.9213 - val_loss: 3.8446 - val_acc: 0.2750\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4307 - acc: 0.9101 - val_loss: 3.9172 - val_acc: 0.2750\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4030 - acc: 0.9326 - val_loss: 3.9301 - val_acc: 0.2750\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4065 - acc: 0.9213 - val_loss: 4.0461 - val_acc: 0.2750\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.3792 - acc: 0.9242 - val_loss: 4.1102 - val_acc: 0.2750\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.3538 - acc: 0.9438 - val_loss: 4.1662 - val_acc: 0.2500\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.3350 - acc: 0.9298 - val_loss: 4.1378 - val_acc: 0.2250\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2740 - acc: 0.9719 - val_loss: 4.2793 - val_acc: 0.2750\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.3192 - acc: 0.9579 - val_loss: 4.2546 - val_acc: 0.2750\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.3024 - acc: 0.9579 - val_loss: 4.2524 - val_acc: 0.2750\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.2645 - acc: 0.9635 - val_loss: 4.2723 - val_acc: 0.2750\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.2806 - acc: 0.9466 - val_loss: 4.3466 - val_acc: 0.2750\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.2120 - acc: 0.9803 - val_loss: 4.4125 - val_acc: 0.2750\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.2330 - acc: 0.9719 - val_loss: 4.4222 - val_acc: 0.2500\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.2058 - acc: 0.9831 - val_loss: 4.4723 - val_acc: 0.2500\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2041 - acc: 0.9719 - val_loss: 4.5115 - val_acc: 0.2750\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1735 - acc: 0.9916 - val_loss: 4.5813 - val_acc: 0.2750\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1834 - acc: 0.9803 - val_loss: 4.5919 - val_acc: 0.2500\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1571 - acc: 0.9972 - val_loss: 4.5910 - val_acc: 0.2500\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1559 - acc: 0.9831 - val_loss: 4.5927 - val_acc: 0.2500\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1416 - acc: 0.9831 - val_loss: 4.6234 - val_acc: 0.2250\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1447 - acc: 0.9860 - val_loss: 4.6572 - val_acc: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      " - 0s - loss: 0.1567 - acc: 0.9803 - val_loss: 4.5709 - val_acc: 0.2750\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.1481 - acc: 0.9888 - val_loss: 4.6624 - val_acc: 0.2250\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.1420 - acc: 0.9860 - val_loss: 4.7283 - val_acc: 0.2750\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.1302 - acc: 0.9860 - val_loss: 4.7703 - val_acc: 0.2500\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.1240 - acc: 0.9888 - val_loss: 4.7983 - val_acc: 0.2250\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.1190 - acc: 0.9944 - val_loss: 4.8010 - val_acc: 0.2250\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.1117 - acc: 0.9916 - val_loss: 4.8301 - val_acc: 0.2500\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.1006 - acc: 0.9972 - val_loss: 4.8344 - val_acc: 0.2500\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0982 - acc: 0.9888 - val_loss: 4.8904 - val_acc: 0.2500\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0886 - acc: 0.9944 - val_loss: 4.9192 - val_acc: 0.2250\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 4.9270 - val_acc: 0.2250\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0932 - acc: 0.9944 - val_loss: 4.9077 - val_acc: 0.2500\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0713 - acc: 1.0000 - val_loss: 4.9203 - val_acc: 0.2250\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0731 - acc: 0.9972 - val_loss: 4.9604 - val_acc: 0.2500\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0812 - acc: 0.9972 - val_loss: 4.9740 - val_acc: 0.2500\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0755 - acc: 0.9972 - val_loss: 4.9454 - val_acc: 0.2750\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 4.9804 - val_acc: 0.2250\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0799 - acc: 0.9944 - val_loss: 5.0155 - val_acc: 0.2250\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0703 - acc: 0.9972 - val_loss: 5.0417 - val_acc: 0.2250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x10fbd3518>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "# fit the model\n",
    "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict character\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += char\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
