{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random \n",
    "\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/lin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n",
      "\n",
      "The type of all_positive_tweets is:  <class 'list'>\n",
      "The type of a tweet entry is:  <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloads sample twitter dataset. uncomment the line below if running on a local machine.\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "\n",
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "\n",
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))\n",
    "\n",
    "\n",
    "#download the stopwords from NLTK\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "\u001b[92mMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "\u001b[94m\n",
      "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n"
     ]
    }
   ],
   "source": [
    "# Our selected sample. Complex enough to exemplify each step\n",
    "tweet = all_positive_tweets[2277]\n",
    "print(tweet)\n",
    "\n",
    "print('\\033[92m' + tweet)\n",
    "print('\\033[94m')\n",
    "\n",
    "# remove old style retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "# remove hyperlinks\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "print(tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92mMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n",
      "\u001b[94m\n",
      "\n",
      "Tokenized string:\n",
      "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('\\033[92m' + tweet2)\n",
    "print('\\033[94m')\n",
    "\n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize tweets\n",
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "print()\n",
    "print('Tokenized string:')\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuation\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Import the english stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92m\n",
      "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n",
      "\u001b[94m\n",
      "removed stop words and punctuation:\n",
      "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweet_tokens)\n",
    "print('\\033[94m')\n",
    "\n",
    "tweets_clean = []\n",
    "\n",
    "for word in tweet_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stopwords_english and  # remove stopwords\n",
    "        word not in string.punctuation):  # remove punctuation\n",
    "        tweets_clean.append(word)\n",
    "\n",
    "print('removed stop words and punctuation:')\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92m\n",
      "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n",
      "\u001b[94m\n",
      "stemmed words:\n",
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweets_clean)\n",
    "print('\\033[94m')\n",
    "\n",
    "# Instantiate stemming class\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Create an empty list to store the stems\n",
    "tweets_stem = [] \n",
    "\n",
    "for word in tweets_clean:\n",
    "    stem_word = stemmer.stem(word)  # stemming word\n",
    "    tweets_stem.append(stem_word)  # append to the list\n",
    "\n",
    "print('stemmed words:')\n",
    "print(tweets_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import utils\n",
    "# from utils import process_tweet # Import the process_tweet function\n",
    "\n",
    "# # choose the same tweet\n",
    "# tweet = all_positive_tweets[2277]\n",
    "\n",
    "# print()\n",
    "# print('\\033[92m')\n",
    "# print(tweet)\n",
    "# print('\\033[94m')\n",
    "\n",
    "# # call the imported function\n",
    "# tweets_stem = process_tweet(tweet); # Preprocess a given tweet\n",
    "\n",
    "# print('preprocessed tweet:')\n",
    "# print(tweets_stem) # Print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to download the full dataset on your own and choose your own set of word embeddings, please see the instructions and some helper code.\n",
    "\n",
    "Download the dataset from this page.(https://code.google.com/archive/p/word2vec/)\n",
    "Search in the page for 'GoogleNews-vectors-negative300.bin.gz' and click the link to download.\n",
    "Copy-paste the code below and run it on your local machine after downloading the dataset to the same directory as the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import nltk\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "# f = open('capitals.txt', 'r').read()\n",
    "# set_words = set(nltk.word_tokenize(f))\n",
    "# select_words = words = ['king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "# for w in select_words:\n",
    "#     set_words.add(w)\n",
    "\n",
    "# def get_word_embeddings(embeddings):\n",
    "\n",
    "#     word_embeddings = {}\n",
    "#     for word in embeddings.vocab:\n",
    "#         if word in set_words:\n",
    "#             word_embeddings[word] = embeddings[word]\n",
    "#     return word_embeddings\n",
    "\n",
    "\n",
    "# # Testing your function\n",
    "# word_embeddings = get_word_embeddings(embeddings)\n",
    "# print(len(word_embeddings))\n",
    "# pickle.dump( word_embeddings, open( \"word_embeddings_subset.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np                         # Linear algebra library\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "from sklearn.decomposition import PCA      # PCA library\n",
    "import pandas as pd                        # Data frame library\n",
    "import math                                # Library for math functions\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFtRJREFUeJzt3XuMlfd95/H3N4MGtwSSUANxbE+gMCnyJshWRgYLZePdGJVkJduq0thZ0BIpa6u1oirrTSQQKGycoLiJSlIp0W5NWq1z6YYkSieoUBzilraywGssXJDTOuBLbbBl3E3sXG0H+t0/5uCOxzNznofznOvzfkmjOZefz/PVAJ/5+ff8LpGZSJLq5XXdLkCS1HmGvyTVkOEvSTVk+EtSDRn+klRDhr8k1ZDhL0k1ZPhLUg0Z/pJUQ3O6XcBMLr744ly6dGm3y5CkvvLggw/+S2YuatauZ8N/6dKlHDlypNtlSFJfiYh/LtLOYR9JqiHDX5JqyPCXpBoy/CWphgx/Saohw1+Sasjwl6QaMvwlqYZ6dpGXJF2I8aOn+dw9j/D087/kLW/8NT7+27/FjVdd2u2yeo7hL2lgrNiyl7P5b89PP/9LtnznOIC/AKZw2EfSQFi6+dXBf94vf3WOz93zSOcL6nGGv6S+Nn70NEs37521zdPP/7JD1fQPh30k9a1t48f52uEnm7Z7yxt/rQPV9Bd7/pL6UtHgB/j4b/9Wm6vpP4a/pL5TJvjBm73TcdhHUl/ZsOsQ9z36o8Ltv3DTlW2spn8Z/pL6xuodB3j2py8Xbv+Fm6601z8Dw19SX1i1fT8/eelc4fZP3Pmf2lhN/3PMX1LPW7fzoMFfMXv+knra1FW7s5kTcPIzBn8R9vwl9ayZVu1OZ8n8YYO/BMNfUs8psmp3stHF87h/67o2VjR4HPaR1FPKzuHfuGaET9/4jjZWNJgMf0k9w6mcnWP4S+oJK7fu48VzBQf4MfhbZfhL6rpV2/eXCn6ncrbOG76SuqrM4q3A4K+K4S+pa1bvOFA4+JfMH+Zxg78ylYR/RKyPiEci4mREbJ6l3fsjIiNirIrrSupfq7bvL3xzd+3yhU7lrFjL4R8RQ8CXgPcCVwAfjIgrpmk3H/gD4P5Wrympvy3dvLdwj3/t8oV8/ZZr2lxR/VTR878aOJmZj2Xmy8A3gBumafcp4LPAixVcU1IfKrt4y+BvnyrC/1LgqUnPTzVee0VEXAVcnpl/OdsHRcStEXEkIo4899xzFZQmqVds2HWIj+5+qHD7jWtGDP42qiL8Y5rXXpmzFRGvAz4P/PdmH5SZd2XmWGaOLVq0qILSJPWCdTsPljqAZcn8YVfttlkV4X8KuHzS88uApyc9nw+8HTgYEU8Aa4A93vSV6mHb+HFOnPl54fYL5g55c7cDqljk9QAwGhHLgNPAzcB/Pv9mZr4AXHz+eUQcBD6WmUcquLakHlb2yEXH+Dun5fDPzLMR8RHgHmAI+LPMfDgi7gCOZOaeVq8hqf+U2Ycf3K6h0yrZ3iEz9wH7prz2iRnaXlvFNSX1rjIzesBVu93gCl9JlSoT/BcNhcHfJYa/pMqUDf5/2vG+Nlaj2Rj+kiqxrETwL5g7ZPB3mVs6S2pZmR7/grlDHPvk+jZWoyIMf0kXrOyRi0vmDzuHv0cY/pIuyLqdB0st3hpdPI8Dt1/bvoJUimP+kkrbsOtQqeBfu3yhwd9j7PlLKqXMqt0AD2DpUYa/pMJW7zhQ+AAWMPh7mcM+kgpZt/NgqeB38VZvs+cvqakyPX6HevqD4S9pVss276Xo/mzO6OkfDvtImtb5IxeLBv/GNSMGfx+x5y/pNcaPni515KLbMfcfe/6SXqVs8G9cM2Lw9yF7/pJeUXYq59rlCz1rt0/Z85cElJ/KuXHNiEcu9jF7/pIYP3q61HYNjvH3P8NfqrkyO3N6wPrgMPylGlu1fT8/eelcobZzAoN/gBj+Uk155GK9ecNXqqEyRy6OLp5n8A8gw1+qmRVbiq/aXTJ/2FW7A8rwl2ri/HYNZwsmv0cuDjbH/KUaKHvkorN6Bp/hLw24ssHvHP56MPylAVZ2uwb36akPx/ylAXUhwe8+PfVhz18aQBt2HSoc/HMCTn7Gk7fqppKef0Ssj4hHIuJkRGye5v3bI+IHEXEsIu6NiLdWcV1Jr7Vq+37ue/RHhdqOLp5n8NdUy+EfEUPAl4D3AlcAH4yIK6Y0OwqMZeYq4NvAZ1u9rqTXWrl1X+HtGtYuX+gc/hqroud/NXAyMx/LzJeBbwA3TG6QmX+Tmb9oPD0MXFbBdSVNsm7nQV48V2wS/5L5w07lrLkqwv9S4KlJz081XpvJh4G/quC6khpW7zhQeDqni7cE1dzwjWlem7b7EREbgTHg3TO8fytwK8DIyEgFpUmDb8UWV+2qvCp6/qeAyyc9vwx4emqjiLgO2Apcn5kvTfdBmXlXZo5l5tiiRYsqKE0abCu37isc/BcNhcGvV1QR/g8AoxGxLCKGgZuBPZMbRMRVwJ8wEfxnKrimVHurtu8vPMa/YO6QO3PqVVoO/8w8C3wEuAf4R+CbmflwRNwREdc3mn0OeD3wrYh4KCL2zPBxkpo4v0Fb0Vk9G9eMcOyT69tclfpNJYu8MnMfsG/Ka5+Y9Pi6Kq4j1V2ZfXoc39dsXOEr9YmVW/cVHuaZExj8mpXhL/WBMjN6PHJRRbixm9TjyhzA4o1dFWX4Sz2szCHrS+YPe2NXhTnsI/WgbePH+drhJwu3H108z316VIrhL/WYDbsOFd6VEzxyURfGYR+ph4wfPW3wqyPs+Us9omyP35O31ArDX+oBZebwAzxxpwewqDWGv9RlyzbvnX4b3Gl45KKq4pi/1EUrt+4rHPxg8Ks6hr/UJWW3a3CoR1Vy2EfqgjJDPW7XoHaw5y910PntmMuM8Rv8agd7/lKHlJ3KaY9f7WT4Sx1QZh9+cLsGtZ/DPlKbbdh1qFTwb1wzYvCr7Qx/qY1ctate5bCP1Cbbxo+XCv4v3HQlN151aRsrkv6N4S+1QZkefwCPO4dfHWb4SxUrs3jLG7vqFsNfqlCZs3bnBAa/usYbvlJFygQ/uE+Pusvwl1p0ftVu0eBfMn/YfXrUdQ77SC24kFW7929d18aKpGIMf+kClV2163YN6iUO+0gXoGzwr12+0OBXT7HnL5W0avt+fvLSucLtXbWrXmT4SyWUDX5X7apXGf5SQWWCf8n8YW/sqqdVEv4RsR74Y2AI+HJm3jnl/bnAV4B3Av8PuCkzn6ji2jO6+3p4/G/begnVRyb8A8DcAo0D4lfA/2hrSRpYAb9zF6z6QFuv0vIN34gYAr4EvBe4AvhgRFwxpdmHgR9n5grg88AftnrdWRn8qlhEia9uF6s+l/CdW+DYN9t6lSpm+1wNnMzMxzLzZeAbwA1T2twA3N14/G3gPRHRvn8jBr+kfnfvHW39+CrC/1LgqUnPTzVem7ZNZp4FXgB+Y+oHRcStEXEkIo4899xzFZQmSX3qhVNt/fgqwn+6HvzUhe5F2pCZd2XmWGaOLVq0qILSJKlPveGytn58FeF/Crh80vPLgKdnahMRc4A3AMXXxJe17N1t+2hJ6oj3fKKtH19F+D8AjEbEsogYBm4G9kxpswfY1Hj8fuCvM7PE/oclbdrjLwCVlkzM6in81e2CNaACfmdX22f7tDzVMzPPRsRHgHuYmOr5Z5n5cETcARzJzD3AnwJfjYiTTPT4b271uk1tmvr7R5rZ0s17C7d1Dr8GQSXz/DNzH7BvymufmPT4ReB3q7iWVLVlJYJ/TmDwayC4sZtqbd3Og6WGbzyARYPC7R1UW8s27y0c/G7HrEFj+KuWyozxuzmbBpHDPqqV80cuFrV2+UKDXwPJ8FdtjB89zUd3P1S4/ejieXz9lmvaWJHUPQ77qBa2jR/na4efLNx+dPE8Dtx+bfsKkrrM8NfAW7FlL2dLTOlZu3yhPX4NPMNfA23l1n2lgt+bu6oLx/w1sNbtPMiL54onv8GvOrHnr4FU9qzdJ+508ZbqxZ6/Bs66nQcNfqkJw18DZcOuQ5w48/PC7Q1+1ZXDPhoYq3cc4Nmfvlyords1qO4Mfw2EMmP8zuGXDH/1ufGjp/lvux8qvEGbwS9NMPzVt8qu2l0yf9jglxoMf/WlDbsOcd+jxY+B9vQt6dWc7aO+s238eKng37hmxOCXprDnr76ybufBwlM5F8wd4tgn17e5Iqk/Gf7qGyu37iu8XYPDPNLsDH/1hTIHsDijR2rOMX/1PINfqp7hr562Ykvx4Hcqp1Sc4a+edP6s3aJ78Y8unucYv1SCY/7qOR65KLWf4a+eciGLtwx+qTzDXz2jzBx+8KxdqRWO+asnlA3+jWtGDH6pBfb81XVlj1z0rF2pdS2Ff0QsBHYDS4EngA9k5o+ntLkS+J/AAuAcsCMzd7dyXQ2OFVuKz+gBT96SqtLqsM9m4N7MHAXubTyf6hfAf8nMfwesB74QEW9s8boaAGWmci6ZP2zwSxVqNfxvAO5uPL4buHFqg8z8YWaeaDx+GjgDLGrxuupzy0qs2nWfHql6rYb/ksx8BqDxffFsjSPiamAYeLTF66qPrdiyt/DJWwa/1B5Nx/wj4vvAm6d5a2uZC0XEJcBXgU2Z+a8ztLkVuBVgZGSkzMerT5QZ4zf4pfZpGv6Zed1M70XEsxFxSWY+0wj3MzO0WwDsBbZl5uFZrnUXcBfA2NhYiduA6nVlF29tXDPCp298Rxsrkuqt1amee4BNwJ2N79+d2iAihoG/AL6Smd9q8XrqQ6t3HODZn75cuL1TOaX2a3XM/05gXUScANY1nhMRYxHx5UabDwD/HvhQRDzU+LqyxeuqT5QN/o1rRgx+qQMiszdHV8bGxvLIkSPdLkMtcPGW1HkR8WBmjjVr5wpftUWZ4J8TcPIzzuGXOsnwV+WWbS4+ldPgl7rD8Felyhy5CAa/1C3u6qnKlAn+i4bC7RqkLjL8VYkywb92+UL+acf72liNpGYMf7WsbPC7D7/UfY7564KVPWvXVbtS7zD8dUEuZPGWwS/1DsNfpZU9gMXFW1LvMfxVysqt+zx5SxoA3vBVYau27+fFc8WT3+CXepc9fxVSZtUuGPxSr7Pnr6bKnLwVGPxSPzD8NauyJ289bvBLfcFhH82ozFDP6OJ5HLj92naWI6lC9vw1rTJDPWuXLzT4pT5j+OtVxo+eZunm4kM9G9eMuF2D1Icc9tErym7X4OItqX8Z/gImFm+VmcNv8Ev9zfBXqV05A5zRIw0Ax/xrrkzwzwmDXxoUhn+NLSsZ/B65KA0Oh31qqmyP3+CXBos9/xoqe9auwS8NHsO/ZsoE/5L5w561Kw0oh31qouwcfs/alQab4V8DG3Yd4r5Hf1S4vXP4pcHnsM+A2zZ+3OCX9Br2/AfYup0HOXHm54XbG/xSfRj+A6ps8HsAi1QvLQ37RMTCiDgQESca3980S9sFEXE6Ir7YyjXV3OodBwoHvydvSfXU6pj/ZuDezBwF7m08n8mngL9t8XpqYvWOAzz705cLtfXkLam+Wg3/G4C7G4/vBm6crlFEvBNYAnyvxetpFqu27y8V/PdvXdfmiiT1qlbDf0lmPgPQ+L54aoOIeB3wR8DHW7yWZrFy6z5+8tK5Qm1HF88z+KWaa3rDNyK+D7x5mre2FrzGbcC+zHwqIppd61bgVoCRkZGCH19v40dP89HdDxVuPyfwyEVJzcM/M6+b6b2IeDYiLsnMZyLiEuDMNM2uAd4VEbcBrweGI+Jnmfma+wOZeRdwF8DY2Fjxk0VqquyqXTdok3Req8M+e4BNjcebgO9ObZCZGzJzJDOXAh8DvjJd8Kuc8aOnSwX/xjUjBr+kV7Q6z/9O4JsR8WHgSeB3ASJiDPi9zPyvLX6+plFmRs+CuUMc++T6Nlckqd9EZm+OroyNjeWRI0e6XUbPWbFlL2cL/pFdNBTuyinVTEQ8mJljzdq5t08f2bDrUOHgdztmSbMx/PtEmQ3anMMvqRn39ukDK7fu48Vzxbr8C+YOGfySmjL8e1zZs3a9uSupCId9etT40dOlgj9wDr+k4uz596Cy2zE7xi+pLMO/x3gAi6ROcNinh2zYdahU8K9dvtDgl3RB7Pn3iDIzemAi+L9+yzVtrEjSIDP8e0DZ4HeoR1KrDP8u2zZ+vFTwe+SipCo45t9l/+f+pwq186xdSVUy/LvsXIGN9UYXz/OsXUmVMvy7bKjJ6WYL5g558pakyhn+XfbB1ZfP+N5FQ+F2DZLawvDvsk/f+A42rhlhav9/45oRt2SW1DYe5iJJA8TDXCRJMzL8JamGDH9JqiHDX5JqyPCXpBoy/CWphgx/Saohw1+SaqhnF3lFxHPAP3e7jmlcDPxLt4sooF/qBGtth36pE6y1am/NzEXNGvVs+PeqiDhSZPVct/VLnWCt7dAvdYK1dovDPpJUQ4a/JNWQ4V/eXd0uoKB+qROstR36pU6w1q5wzF+SasievyTVkOHfREQsjIgDEXGi8f1NM7QbiYjvRcQ/RsQPImJpZystXmuj7YKIOB0RX+xkjY1rN60zIq6MiEMR8XBEHIuImzpY3/qIeCQiTkbE5mnenxsRuxvv39+NP+tJtTSr9fbG38djEXFvRLy1G3U2apm11knt3h8RGRFdmVVTpM6I+EDj5/pwRPx5p2usRGb6NcsX8Flgc+PxZuAPZ2h3EFjXePx64Nd7tdbG+38M/DnwxV6sE3gbMNp4/BbgGeCNHahtCHgU+E1gGPgH4IopbW4D/lfj8c3A7k7/DEvU+h/O/10Efr+Xa220mw/8HXAYGOvFOoFR4Cjwpsbzxd34mbb6Zc+/uRuAuxuP7wZunNogIq4A5mTmAYDM/Flm/qJzJb6iaa0AEfFOYAnwvQ7VNVXTOjPzh5l5ovH4aeAM0HThSgWuBk5m5mOZ+TLwjUa9k02u/9vAeyJi6kmcndC01sz8m0l/Fw8Dl3W4xvOK/FwBPsVE5+DFThY3SZE6bwG+lJk/BsjMMx2usRKGf3NLMvMZgMb3xdO0eRvwfER8JyKORsTnImKoo1VOaFprRLwO+CPg4x2ubbIiP9NXRMTVTPTCHu1AbZcCT016fqrx2rRtMvMs8ALwGx2obaoitU72YeCv2lrRzJrWGhFXAZdn5l92srApivxM3wa8LSLui4jDEbG+Y9VVaE63C+gFEfF94M3TvLW14EfMAd4FXAU8CewGPgT8aRX1TVZBrbcB+zLzqXZ2Viuo8/znXAJ8FdiUmf9aRW3NLjnNa1OnxBVp0wmF64iIjcAY8O62VjSzWWttdEo+z8S/m24q8jOdw8TQz7VM/J/U30fE2zPz+TbXVinDH8jM62Z6LyKejYhLMvOZRhBN9794p4CjmflY478ZB9bQhvCvoNZrgHdFxG1M3JsYjoifZeaMN+C6VCcRsQDYC2zLzMNV1jeLU8Dlk55fBjw9Q5tTETEHeAPwo86UN20d501XKxFxHRO/dN+dmS91qLapmtU6H3g7cLDRKXkzsCcirs/MIx2rsvif/+HM/BXweEQ8wsQvgwc6U2I1HPZpbg+wqfF4E/Ddado8ALwpIs6PSf9H4AcdqG2qprVm5obMHMnMpcDHgK9UHfwFNK0zIoaBv2Civm91sLYHgNGIWNao4WYm6p1scv3vB/46G3f+OqxprY2hlD8Bru/y2PSstWbmC5l5cWYubfzdPMxEzZ0M/qZ1NowzcSOdiLiYiWGgxzpaZRW6fce517+YGMu9FzjR+L6w8foY8OVJ7dYBx4DjwP8Ghnu11kntP0R3Zvs0rRPYCPwKeGjS15Udqu99wA+ZuMewtfHaHUyEEcBFwLeAk8D/BX6zi38/m9X6feDZST/DPb1a65S2B+nCbJ+CP9MAdjLRwTsO3Nytn2krX67wlaQacthHkmrI8JekGjL8JamGDH9JqiHDX5JqyPCXpBoy/CWphgx/Saqh/w+53EM+ujqt3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 1  # The amount of the correlation\n",
    "x = np.random.uniform(1,2,1000) # Generate 1000 samples from a uniform random variable\n",
    "y = x.copy() * n # Make y = n * x\n",
    "\n",
    "# PCA works better if the data is centered\n",
    "x = x - np.mean(x) # Center x. Remove its mean\n",
    "y = y - np.mean(y) # Center y. Remove its mean\n",
    "\n",
    "data = pd.DataFrame({'x': x, 'y': y}) # Create a data frame with x and y\n",
    "plt.scatter(data.x, data.y) # Plot the original correlated data in blue\n",
    "\n",
    "pca = PCA(n_components=2) # Instantiate a PCA. Choose to get 2 output variables\n",
    "\n",
    "# Create the transformation model for this data. Internally, it gets the rotation \n",
    "# matrix and the explained variance\n",
    "pcaTr = pca.fit(data)\n",
    "\n",
    "rotatedData = pcaTr.transform(data) # Transform the data base on the rotation matrix of pcaTr\n",
    "# # Create a data frame with the new variables. We call these new variables PC1 and PC2\n",
    "dataPCA = pd.DataFrame(data = rotatedData, columns = ['PC1', 'PC2']) \n",
    "\n",
    "# Plot the transformed data in orange\n",
    "plt.scatter(dataPCA.PC1, dataPCA.PC2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
