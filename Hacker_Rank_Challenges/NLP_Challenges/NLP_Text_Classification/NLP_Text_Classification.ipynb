{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
    "\n",
    "Formally, given a training sample of tweets and labels, where label ‘1’ denotes the tweet is racist/sexist and label ‘0’ denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "# from keras import layers, models, optimizers\n",
    "# from keras.preprocessing import text, sequence\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "for line in corpus.split('\\n'):\n",
    "    corpus_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_length = len('__label__2')\n",
    "y_list = []\n",
    "X_list = []\n",
    "for document in corpus_list:\n",
    "    y_list.append(document[:label_length])\n",
    "    X_list.append(document[label_length+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_y_list = []\n",
    "for i in y_list:\n",
    "    if i == '__label__2':\n",
    "        processed_y_list.append(2)\n",
    "    else:\n",
    "        processed_y_list.append(1)\n",
    "y_list = processed_y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = np.array(y_list)\n",
    "\n",
    "y_list = y_list.reshape(-1, 1)\n",
    "\n",
    "y_list = y_list.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X_list, y_list, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectors As Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# X_vectorized = vectorizer.fit_transform(X_list)\n",
    "vectorizer.fit(X_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train = vectorizer.transform(X_train)\n",
    "tfidf_X_validation = vectorizer.transform(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding As Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pretrained model from Spacy\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "word2vec_X_train = [nlp(sentence) for sentence in X_train]\n",
    "\n",
    "word2vec_X_validation = [nlp(sentence) for sentence in X_validation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_X_train = [sentence.vector for sentence in word2vec_X_train]\n",
    "\n",
    "word2vec_X_validation = [sentence.vector for sentence in word2vec_X_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_X_train = np.array(word2vec_X_train)\n",
    "\n",
    "word2vec_X_validation = np.array(word2vec_X_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(classifier, X_train, X_validation, y_train, y_validation):\n",
    "    model = classifier\n",
    "    model.fit(X_train, y_train)\n",
    "    y_validation_predict = model.predict(X_validation)\n",
    "    report = classification_report(y_validation_predict, y_validation)\n",
    "    accuracy = round(accuracy_score(y_validation_predict, y_validation),2)\n",
    "    print(report)\n",
    "    print('accuracy is', accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.84      0.85      1006\n",
      "           2       0.84      0.86      0.85       994\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n",
      "accuracy is 0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(LogisticRegression(), tfidf_X_train, tfidf_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.67      0.69      1057\n",
      "           2       0.65      0.69      0.67       943\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2000\n",
      "   macro avg       0.68      0.68      0.68      2000\n",
      "weighted avg       0.68      0.68      0.68      2000\n",
      "\n",
      "accuracy is 0.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(LogisticRegression(), word2vec_X_train, word2vec_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.73      0.79      1154\n",
      "           2       0.70      0.83      0.76       846\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      2000\n",
      "   macro avg       0.78      0.78      0.77      2000\n",
      "weighted avg       0.79      0.78      0.78      2000\n",
      "\n",
      "accuracy is 0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(RandomForestClassifier(), tfidf_X_train, tfidf_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.58      0.65      1249\n",
      "           2       0.48      0.64      0.55       751\n",
      "\n",
      "   micro avg       0.60      0.60      0.60      2000\n",
      "   macro avg       0.61      0.61      0.60      2000\n",
      "weighted avg       0.64      0.60      0.61      2000\n",
      "\n",
      "accuracy is 0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(RandomForestClassifier(), word2vec_X_train, word2vec_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.76      0.79      1079\n",
      "           2       0.74      0.81      0.78       921\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      2000\n",
      "   macro avg       0.78      0.79      0.78      2000\n",
      "weighted avg       0.79      0.78      0.78      2000\n",
      "\n",
      "accuracy is 0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(GradientBoostingClassifier(), tfidf_X_train, tfidf_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.66      0.69      1091\n",
      "           2       0.63      0.70      0.66       909\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2000\n",
      "   macro avg       0.68      0.68      0.68      2000\n",
      "weighted avg       0.68      0.68      0.68      2000\n",
      "\n",
      "accuracy is 0.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(GradientBoostingClassifier(), word2vec_X_train, word2vec_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to do a gridsearch on gradient boosting classifier:\n",
    "\n",
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "#     \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"max_depth\":[3,5,8],\n",
    "#     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     \"n_estimators\":[10]\n",
    "#     }\n",
    "\n",
    "# clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_jobs=-1)\n",
    "\n",
    "# clf.fit(trainX, trainY)\n",
    "# print(clf.score(trainX, trainY))\n",
    "# print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-4, 4, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.83      0.84      1013\n",
      "           2       0.83      0.84      0.84       987\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n",
      "accuracy is 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training(MLPClassifier(), tfidf_X_train, tfidf_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.67      0.64       916\n",
      "           2       0.70      0.65      0.67      1084\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      2000\n",
      "   macro avg       0.66      0.66      0.66      2000\n",
      "weighted avg       0.66      0.66      0.66      2000\n",
      "\n",
      "accuracy is 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(MLPClassifier(), word2vec_X_train, word2vec_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler(with_mean=False)  \n",
    "\n",
    "scaler.fit(tfidf_X_train) \n",
    "\n",
    "mlp_tfidf_X_train = scaler.transform(tfidf_X_train)  \n",
    "\n",
    "mlp_tfidf_X_validation = scaler.transform(tfidf_X_validation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.77      0.78      1030\n",
      "           2       0.76      0.79      0.78       970\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      2000\n",
      "   macro avg       0.78      0.78      0.78      2000\n",
      "weighted avg       0.78      0.78      0.78      2000\n",
      "\n",
      "accuracy is 0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(MLPClassifier(), mlp_tfidf_X_train, mlp_tfidf_X_validation, y_train, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
