{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# import sys\n",
    "# if not sys.warnoptions:\n",
    "#     import warnings\n",
    "#     warnings.simplefilter(\"ignore\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>champion products ch approves stock split cham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>computer terminal systems cpml completes sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>am international inc am nd qtr jan oper shr lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0        1  champion products ch approves stock split cham...\n",
       "1        2  computer terminal systems cpml completes sale ...\n",
       "2        1  cobanco inc cbco year net shr cts vs dlrs net ...\n",
       "3        1  am international inc am nd qtr jan oper shr lo...\n",
       "4        1  brown forman inc bfd th qtr net shr one dlr vs..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_fwf('trainingdata.txt', delim_whitespace = True, header = None)\n",
    "\n",
    "df = df.rename(columns = {df.columns[0]:'text'})\n",
    "df = df[['text']][1:]\n",
    "\n",
    "df = pd.DataFrame(df['text'].str.split(\" \", 1).tolist(), columns = ['category','text'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>champion products ch approves stock split cham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>computer terminal systems cpml completes sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>am international inc am nd qtr jan oper shr lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0        1  champion products ch approves stock split cham...\n",
       "1        2  computer terminal systems cpml completes sale ...\n",
       "2        1  cobanco inc cbco year net shr cts vs dlrs net ...\n",
       "3        1  am international inc am nd qtr jan oper shr lo...\n",
       "4        1  brown forman inc bfd th qtr net shr one dlr vs..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change to lower case\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# remove punctuation\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['text'] = df['text'].str.replace(punct_sign, '')\n",
    "    \n",
    "# remove possessive pronoun terminations\n",
    "df['text'] = df['text'].str.replace(\"'s\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/lin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['text']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "df['text'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>champion products ch approve stock split champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>computer terminal systems cpml complete sale c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>be international inc be nd qtr jan oper shr lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0        1  champion products ch approve stock split champ...\n",
       "1        2  computer terminal systems cpml complete sale c...\n",
       "2        1  cobanco inc cbco year net shr cts vs dlrs net ...\n",
       "3        1  be international inc be nd qtr jan oper shr lo...\n",
       "4        1  brown forman inc bfd th qtr net shr one dlr vs..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "for stop_word in stop_words:\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['text'] = df['text'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>champion products ch approve stock split champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>computer terminal systems cpml complete sale c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>international inc  nd qtr jan oper shr loss t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0        1  champion products ch approve stock split champ...\n",
       "1        2  computer terminal systems cpml complete sale c...\n",
       "2        1  cobanco inc cbco year net shr cts vs dlrs net ...\n",
       "3        1   international inc  nd qtr jan oper shr loss t...\n",
       "4        1  brown forman inc bfd th qtr net shr one dlr vs..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], \n",
    "                                                    df['category'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "# min_df = 10\n",
    "# max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4662, 300)\n",
      "(823, 300)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "#                         max_df=max_df,\n",
    "#                         min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'C': 1.0,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'max_iter': 100,\n",
      " 'multi_class': 'warn',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': 1,\n",
      " 'solver': 'warn',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state = 1)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(lr.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [0.1,\n",
      "       0.2,\n",
      "       0.30000000000000004,\n",
      "       0.4,\n",
      "       0.5,\n",
      "       0.6,\n",
      "       0.7000000000000001,\n",
      "       0.8,\n",
      "       0.9,\n",
      "       1.0],\n",
      " 'class_weight': ['balanced', None],\n",
      " 'multi_class': ['multinomial'],\n",
      " 'penalty': ['l2'],\n",
      " 'solver': ['newton-cg', 'sag', 'saga', 'lbfgs']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# C\n",
    "C = [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 10)]\n",
    "\n",
    "# multi_class\n",
    "multi_class = ['multinomial']\n",
    "\n",
    "# solver\n",
    "solver = ['newton-cg', 'sag', 'saga', 'lbfgs']\n",
    " \n",
    "# class_weight\n",
    "class_weight = ['balanced', None]\n",
    "\n",
    "# penalty\n",
    "penalty = ['l2']\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'C': C,\n",
    "               'multi_class': multi_class,\n",
    "               'solver': solver,\n",
    "               'class_weight': class_weight,\n",
    "               'penalty': penalty}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=1, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=None,\n",
       "          param_distributions={'C': [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9, 1.0], 'multi_class': ['multinomial'], 'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'], 'class_weight': ['balanced', None], 'penalty': ['l2']},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# First create the base model to tune\n",
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=lr,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'solver': 'sag', 'penalty': 'l2', 'multi_class': 'multinomial', 'class_weight': None, 'C': 1.0}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9455169455169455\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can do a more exhaustive search centered in those values:\n",
    "\n",
    "Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   57.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=2, random_state=1, test_size=0.33, train_size=None),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=8, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.6, 0.6444444444444444, 0.6888888888888889, 0.7333333333333333, 0.7777777777777778, 0.8222222222222222, 0.8666666666666667, 0.9111111111111111, 0.9555555555555555, 1.0], 'multi_class': ['multinomial'], 'solver': ['sag'], 'class_weight': ['balanced'], 'penalty': ['l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "C = [float(x) for x in np.linspace(start = 0.6, stop = 1, num = 10)]\n",
    "multi_class = ['multinomial']\n",
    "solver = ['sag']\n",
    "class_weight = ['balanced']\n",
    "penalty = ['l2']\n",
    "\n",
    "param_grid = {'C': C,\n",
    "               'multi_class': multi_class,\n",
    "               'solver': solver,\n",
    "               'class_weight': class_weight,\n",
    "               'penalty': penalty}\n",
    "\n",
    "# Create a base model\n",
    "lrc = LogisticRegression(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 2, test_size = .33, random_state = 1)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=lrc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "          random_state=8, solver='sag', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lrc = grid_search.best_estimator_\n",
    "\n",
    "best_lrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "          random_state=8, solver='sag', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lrc.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_pred = best_lrc.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "0.9081939081939082\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, best_lrc.predict(features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: \n",
      "0.9125151883353585\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, lrc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.93      0.95       414\n",
      "           2       0.85      0.98      0.91       243\n",
      "           3       0.96      0.96      0.96        54\n",
      "           4       0.36      0.45      0.40        11\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.86      0.93      0.89        41\n",
      "           7       0.87      0.62      0.72        21\n",
      "           8       0.84      0.60      0.70        35\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       823\n",
      "   macro avg       0.72      0.68      0.69       823\n",
      "weighted avg       0.91      0.91      0.91       823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,lrc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation for Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'deviance',\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_iter_no_change': None,\n",
      " 'presort': 'auto',\n",
      " 'random_state': 1,\n",
      " 'subsample': 1.0,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(random_state = 1)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(gb.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  9.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=2, random_state=8, test_size=0.33, train_size=None),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [5, 10, 15], 'max_features': ['sqrt'], 'min_samples_leaf': [2], 'min_samples_split': [50, 100], 'n_estimators': [800], 'learning_rate': [0.1, 0.5], 'subsample': [1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "max_depth = [5, 10, 15]\n",
    "max_features = ['sqrt']\n",
    "min_samples_leaf = [2]\n",
    "min_samples_split = [50, 100]\n",
    "n_estimators = [800]\n",
    "learning_rate = [.1, .5]\n",
    "subsample = [1.]\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'n_estimators': n_estimators,\n",
    "    'learning_rate': learning_rate,\n",
    "    'subsample': subsample\n",
    "\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "gbc = GradientBoostingClassifier(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 2, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=gbc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "              max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=2, min_samples_split=50,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=800,\n",
       "              n_iter_no_change=None, presort='auto', random_state=8,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gbc = grid_search.best_estimator_\n",
    "\n",
    "best_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "              max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=2, min_samples_split=50,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=800,\n",
       "              n_iter_no_change=None, presort='auto', random_state=8,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gbc.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_pred = best_gbc.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "0.9987129987129987\n"
     ]
    }
   ],
   "source": [
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, best_gbc.predict(features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: \n",
      "0.9550425273390036\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, gbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is another document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>documents are seperated by</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         text\n",
       "1         This is a document \n",
       "2   this is another document \n",
       "3  documents are seperated by"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df2 = pd.read_fwf('stdin.txt', delim_whitespace = True, header = None)\n",
    "\n",
    "df2 = df2.rename(columns = {df2.columns[0]:'text'})\n",
    "df2 = df2[['text']][1:]\n",
    "\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to lower case\n",
    "df2['text'] = df2['text'].str.lower()\n",
    "\n",
    "# remove punctuation\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df2['text'] = df2['text'].str.replace(punct_sign, '')\n",
    "    \n",
    "# remove possessive pronoun terminations\n",
    "df2['text'] = df2['text'].str.replace(\"'s\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "for stop_word in stop_words:\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df2['text'] = df2['text'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nrows = len(df2)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['text']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "df2['text'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = df2['text']\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "#                         max_df=max_df,\n",
    "#                         min_df=min_df,\n",
    "                        max_features=300,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_new = tfidf.fit_transform(X_new).toarray()\n",
    "\n",
    "# print(features_new)\n",
    "# best_gbc.predict(features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', '1', '1'], dtype=object)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gbc.predict(features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
