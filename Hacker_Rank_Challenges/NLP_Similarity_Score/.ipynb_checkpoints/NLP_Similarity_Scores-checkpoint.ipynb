{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hackerrank.com/challenges/nlp-similarity-scores/problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "import dateutil.parser\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s= [\"I'd like an apple\", \"An apple a day keeps the doctor away\", \"Never compare an apple to an orange\", \n",
    "\"I prefer scikit-learn to orange\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1),\n",
    "#                               strip_accents='ascii', lowercase=True)\n",
    "\n",
    "# doc1_vectorized = vectorizer.fit_transform(doc1)\n",
    "\n",
    "# vectorized_list = []\n",
    "\n",
    "# index = 1\n",
    "# for doc in s[1:]:\n",
    "#     print(doc)\n",
    "#     vectorized_doc = vectorizer.transform([doc])\n",
    "#     consine_similarity_score = cosine_similarity(doc1_vectorized, vectorized_doc)\n",
    "#     vectorized_list.append((index ,consine_similarity_score))\n",
    "#     index += 1\n",
    "\n",
    "# print(vectorized_list)\n",
    "# sort_vectorized_list = sorted(vectorized_list, key = lambda x: -x[1])\n",
    "\n",
    "# print(sort_vectorized_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #Tokenizing data\n",
    "# gen_docs = [[w.lower() for w in word_tokenize(original_doc)]]\n",
    "\n",
    "#     # Create dictionary\n",
    "# dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "\n",
    "#     # Creat Document-Term Matrix\n",
    "# corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "\n",
    "#     # Creat TF-IDF Model\n",
    "# tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "#     # Creat Similarity Checker\n",
    "# similar_qs = gensim.similarities.Similarity(\"\",tf_idf[corpus],num_features=len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = [document.lower() for document in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for document in processed:\n",
    "    for word in document.split(\" \"):\n",
    "        corpus.append(word)\n",
    "corpus = set(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "tf = {}\n",
    "\n",
    "def get_tf(document):\n",
    "    for (key, freq) in collections.Counter(document.split(\" \")).items():\n",
    "        tf[key] =  freq/len(document)\n",
    "    return tf\n",
    "    \n",
    "print(get_tf(processed[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log, sqrt\n",
    "def get_idf(document):\n",
    "    idf = {}\n",
    "    for word in document.split(\" \"):\n",
    "        document_has_word = 0\n",
    "        for i in range(len(processed)):\n",
    "            if word in processed[i]:\n",
    "                document_has_word += 1\n",
    "        # use smooth inverse document frequency:\n",
    "        idf[word]  = log(len(processed) / ((document_has_word) + 1))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_idf(processed[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(document):\n",
    "    tf = get_tf(document)\n",
    "    idf = get_idf(document)\n",
    "    tf_idf = []\n",
    "    for word in corpus:\n",
    "        current_tf_idf = tf.get(word,0) * idf.get(word,0)\n",
    "        tf_idf.append(current_tf_idf)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tf_idf(processed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_cosine_similarity(document1, document2):\n",
    "    v1 = get_tf_idf(document1)\n",
    "    v2 = get_tf_idf(document2)\n",
    "    numerator = np.dot(v1, v2)\n",
    "    v1norm = (sum(v**2 for v in v1))**0.5\n",
    "    v2norm = (sum(v**2 for v in v2))**0.5\n",
    "    cosine_similarity = numerator/(v1norm * v2norm)\n",
    "    return cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_score_list = []\n",
    "for i in range(1, len(processed)):\n",
    "    score = get_cosine_similarity(processed[0], processed[1])\n",
    "    index = i+1\n",
    "    index_score_list.append((index, score))\n",
    "\n",
    "top_index = sorted(index_score_list, key = lambda x: -x[1])[0][0]\n",
    "\n",
    "print(top_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Combine:\n",
    "import collections\n",
    "import math\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "s= [\"I'd like an apple\", \"An apple a day keeps the doctor away\", \"Never compare an apple to an orange\", \n",
    "\"I prefer scikit-learn to orange\"]\n",
    "\n",
    "processed = [document.lower() for document in s]\n",
    "\n",
    "corpus = []\n",
    "for document in processed:\n",
    "    for word in document.split(\" \"):\n",
    "        corpus.append(word)\n",
    "corpus = set(corpus)\n",
    "\n",
    "\n",
    "# calculate tf\n",
    "def get_tf(document):\n",
    "    tf = {}\n",
    "    for (key, freq) in collections.Counter(document.split(\" \")).items():\n",
    "        tf[key] =  freq/len(document)\n",
    "    return tf\n",
    "\n",
    "# calculate idf\n",
    "def get_idf(document):\n",
    "    idf = {}\n",
    "    for word in document.split(\" \"):\n",
    "        document_has_word = 0\n",
    "        for i in range(len(processed)):\n",
    "            if word in processed[i]:\n",
    "                document_has_word += 1\n",
    "        # use smooth inverse document frequency:\n",
    "        idf[word]  = log(len(processed) / ((document_has_word) + 1))\n",
    "    return idf\n",
    "\n",
    "# calculate tf_idf\n",
    "def get_tf_idf(document):\n",
    "    tf = get_tf(document)\n",
    "    idf = get_idf(document)\n",
    "    tf_idf = []\n",
    "    for word in corpus:\n",
    "        current_tf_idf = tf.get(word,0) * idf.get(word,0)\n",
    "        tf_idf.append(current_tf_idf)\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "def get_cosine_similarity(document1, document2):\n",
    "    v1 = get_tf_idf(document1)\n",
    "    v2 = get_tf_idf(document2)\n",
    "    numerator = np.dot(v1, v2)\n",
    "    v1norm = (sum(v**2 for v in v1))**0.5\n",
    "    v2norm = (sum(v**2 for v in v2))**0.5\n",
    "    cosine_similarity = numerator/(v1norm * v2norm)\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "index_score_list = []\n",
    "for i in range(1, len(processed)):\n",
    "    score = get_cosine_similarity(processed[0], processed[i])\n",
    "    index = i+1\n",
    "    index_score_list.append((index, score))\n",
    "\n",
    "top_index = sorted(index_score_list, key = lambda x: -x[1])[0][0]\n",
    "\n",
    "print(top_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
