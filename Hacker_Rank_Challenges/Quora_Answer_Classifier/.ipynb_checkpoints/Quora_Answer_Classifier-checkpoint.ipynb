{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import sys\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0                                            4500 23\n",
      "1  Nt8FJ +1 1:12087620705283 2:4.797982 3:1 4:4.9...\n",
      "2  VCaTF +1 1:282114466020 2:3.151926 3:1 4:3.737...\n",
      "3  gParY +1 1:173284955 2:1.785813 3:1 4:1.791759...\n",
      "4  DtWDw +1 1:4708728355523 2:2.394989 3:1 4:3.09...\n"
     ]
    }
   ],
   "source": [
    "# when reading locally\n",
    "df = pd.read_csv('input00.txt', header= None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_line = df[0][0]\n",
    "feature_num = int(first_line.split(' ')[1])\n",
    "train_cnt = int(first_line.split(' ')[0])\n",
    "input_cnt = df[0][train_cnt+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_validation = pd.DataFrame(df[1:train_cnt+1][0].str.split(\" \").tolist())\n",
    "\n",
    "df_train, df_validation = train_test_split(df_train_validation, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training_validation(data):\n",
    "    y = data[1].astype(float)\n",
    "    y = y.replace(-1, 0)\n",
    "    X = data.drop(data.columns[[0, 1]], axis=1)\n",
    "    X = X.T.reset_index(drop=True).T\n",
    "    \n",
    "    for i in range(feature_num):\n",
    "        X[i] = X[i].str.split(\":\", expand = True)[1]\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = process_training_validation(df_train)\n",
    "X_validation, y_validation = process_training_validation(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.DataFrame(df[train_cnt+2:][0].str.split(\" \").tolist())\n",
    "\n",
    "input_features = df_input.drop(df_input.columns[0], axis=1)\n",
    "\n",
    "input_features = input_features.T.reset_index(drop=True).T\n",
    "\n",
    "\n",
    "for i in range(feature_num):\n",
    "    input_features[i] = input_features[i].str.split(\":\", expand = True)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(method):\n",
    "    model = method\n",
    "    model.fit(X_train, y_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    print (classification_report(y_validation, y_validation_pred))\n",
    "    print (\"The accuracy score is {:.2%}\".format(accuracy_score(y_validation, y_validation_pred)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       466\n",
      "         1.0       0.48      1.00      0.65       434\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       900\n",
      "   macro avg       0.24      0.50      0.33       900\n",
      "weighted avg       0.23      0.48      0.31       900\n",
      "\n",
      "The accuracy score is 48.22%\n"
     ]
    }
   ],
   "source": [
    "model_LogisticRegression = evaluate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       466\n",
      "         1.0       0.82      0.82      0.82       434\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       900\n",
      "   macro avg       0.82      0.82      0.82       900\n",
      "weighted avg       0.82      0.82      0.82       900\n",
      "\n",
      "The accuracy score is 82.33%\n"
     ]
    }
   ],
   "source": [
    "model_RandomForest = evaluate_model(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      1.00      0.68       466\n",
      "         1.0       0.00      0.00      0.00       434\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       900\n",
      "   macro avg       0.26      0.50      0.34       900\n",
      "weighted avg       0.27      0.52      0.35       900\n",
      "\n",
      "The accuracy score is 51.78%\n"
     ]
    }
   ],
   "source": [
    "model_SGDC = evaluate_model(SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       466\n",
      "         1.0       0.48      1.00      0.65       434\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       900\n",
      "   macro avg       0.24      0.50      0.33       900\n",
      "weighted avg       0.23      0.48      0.31       900\n",
      "\n",
      "The accuracy score is 48.22%\n"
     ]
    }
   ],
   "source": [
    "model_SVC = evaluate_model(LinearSVC(C=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC = evaluate_model(SVC(kernel = 'linear', C=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC is too slow\n",
    "# model5 = SVC( kernel='linear', C=100 )\n",
    "# model5.fit(training_features, training_label)\n",
    "# predictions = model5.predict(validation_features)\n",
    "# print (classification_report(validation_label, predictions))\n",
    "# print (\"The accuracy score is {:.2%}\".format(accuracy_score(validation_label, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = MultinomialNB()\n",
    "model6.fit(training_features, training_label)\n",
    "predictions = model6.predict(validation_features)\n",
    "print (classification_report(validation_label, predictions))\n",
    "print (\"The accuracy score is {:.2%}\".format(accuracy_score(validation_label, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = GaussianNB()\n",
    "model6.fit(training_features, training_label)\n",
    "predictions = model6.predict(validation_features)\n",
    "print (classification_report(validation_label, predictions))\n",
    "print (\"The accuracy score is {:.2%}\".format(accuracy_score(validation_label, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = SGDClassifier( loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=8, tol=None )\n",
    "model6.fit(training_features, training_label)\n",
    "predictions = model6.predict(validation_features)\n",
    "print (classification_report(validation_label, predictions))\n",
    "print (\"The accuracy score is {:.2%}\".format(accuracy_score(validation_label, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.GradientBoostingClassifier( n_estimators=200, max_depth=3, learning_rate=0.3 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [i[0] for i in df[training_cnt+2:][0].str.split(\" \")]\n",
    "\n",
    "prediction_list = model.predict(input_features)\n",
    "\n",
    "final_prediction_list = []\n",
    "\n",
    "\n",
    "for i in range(len(prediction_list)):\n",
    "    if prediction_list[i] == 1:\n",
    "        final_prediction_list.append(name_list[i] + \" \" + '+1')\n",
    "#         print(name_list[i] + \" \" + '+1')\n",
    "    else:\n",
    "        final_prediction_list.append(name_list[i] + \" \" + '-1')\n",
    "#         print(name_list[i] + \" \" + '-1')\n",
    "    \n",
    "# print(final_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0                                                  1\n",
      "0   5 23                                                NaN\n",
      "1  2LuzC  1 1:2101216030446 2:1.807711 3:1 4:4.262680 5:...\n",
      "2  LmnUc  1 1:99548723068 2:3.032810 3:1 4:2.772589 5:2....\n",
      "3  ZINTz  -1 1:3030695193589 2:1.741764 3:1 4:2.708050 5...\n",
      "4  gX60q  1 1:2086220371355 2:1.774193 3:1 4:3.258097 5:...\n",
      "5  5HG4U  -1 1:352013287143 2:1.689824 3:1 4:0.000000 5:...\n",
      "6      2                                                NaN\n",
      "7  PdxMK  1:340674897225 2:1.744152 3:1 4:5.023881 5:7.0...\n",
      "8  ehZ0a  1:2090062840058 2:1.939101 3:1 4:3.258097 5:2....\n"
     ]
    }
   ],
   "source": [
    "# When reading from local files:\n",
    "\n",
    "df = pd.read_fwf('Quora_Answer_Classifier_Input.txt', delim_whitespace = True, header = None)\n",
    "print(df)\n",
    "# first_line = df[0][0]\n",
    "\n",
    "# feature_num = int(first_line.split(' ')[1])\n",
    "# training_cnt = int(first_line.split(' ')[0])\n",
    "# input_cnt = df[0][training_cnt+1]\n",
    "\n",
    "\n",
    "\n",
    "# df_training = pd.DataFrame(df[1:training_cnt+1][1].str.split(\" \").tolist())\n",
    "\n",
    "\n",
    "# for i in range(1, feature_num + 1):\n",
    "#     df_training[i] = df_training[i].str.split(\":\", expand = True)[1]\n",
    "\n",
    "\n",
    "# training_label = df_training[0].astype(float)\n",
    "\n",
    "# training_label = training_label.replace(-1, 0)\n",
    "\n",
    "# training_features = df_training.iloc[:, 1:]\n",
    "\n",
    "\n",
    "# df_input = pd.DataFrame(df[training_cnt+2:][1].str.split(\" \").tolist())\n",
    "\n",
    "\n",
    "# df_input[2].str.split(\":\", expand = True)[1]\n",
    "\n",
    "# for i in range(feature_num):\n",
    "#     df_input[i] = df_input[i].str.split(\":\", expand = True)[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(training_features, training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PdxMK +1', 'ehZ0a +1']\n"
     ]
    }
   ],
   "source": [
    "name_list = [i for i in df[training_cnt+2:][0]]\n",
    "\n",
    "prediction_list = model.predict(df_input)\n",
    "\n",
    "final_prediction_list = []\n",
    "\n",
    "for i in range(len(prediction_list)):\n",
    "    if prediction_list[i] == 1:\n",
    "        final_prediction_list.append(name_list[i] + \" \" + '+1')\n",
    "    else:\n",
    "        final_prediction_list.append(name_list[i] + \" \" + '-1')\n",
    "    \n",
    "print(final_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
